import json
import os
import sys
import random
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List, Optional
from openai import AsyncOpenAI
from openai.types.beta import Thread
from openai.types.beta.threads import (
    MessageContentImageFile,
    MessageContentText,
    ThreadMessage,
)

from openai.types.beta.threads.runs import RunStep
from openai.types.beta.threads.runs.tool_calls_step_details import ToolCall

from chainlit.element import Element
import chainlit as cl

from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.schema import StrOutputParser
from langchain.schema.runnable import Runnable
from langchain.schema.runnable.config import RunnableConfig
from langchain.chains import LLMChain

from google.cloud import vision

# Set up the OpenAI client
api_key = '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'
client = AsyncOpenAI(api_key=api_key)

# Define allowed MIME types
allowed_mime = [
    "application/pdf",
    "image/jpeg",
    "image/png",
    "image/jpg",
    "text/plain",
]

# Check if the files uploaded are allowed
async def check_files(files: List[Element]):
    for file in files:
        if file.mime not in allowed_mime:
            return False
    return True

async def process_tool_call(
    step_references: Dict[str, cl.Step],
    step: RunStep,
    tool_call: ToolCall,
    name: str,
    input: Any,
    output: Any,
    show_input: str = None,
):
    cl_step = None
    update = False
    if not tool_call.id in step_references:
        cl_step = cl.Step(
            name=name,
            type="tool",
            parent_id=cl.context.current_step.id,
            show_input=show_input,
        )
        step_references[tool_call.id] = cl_step
    else:
        update = True
        cl_step = step_references[tool_call.id]

    if step.created_at:
        cl_step.start = datetime.fromtimestamp(step.created_at).isoformat()
    if step.completed_at:
        cl_step.end = datetime.fromtimestamp(step.completed_at).isoformat()
    cl_step.input = input
    cl_step.output = output

    if update:
        await cl_step.update()
    else:
        await cl_step.send()

class DictToObject:
    def __init__(self, dictionary):
        for key, value in dictionary.items():
            if isinstance(value, dict):
                setattr(self, key, DictToObject(value))
            else:
                setattr(self, key, value)

    def __str__(self):
        return "\n".join(f"{key}: {value}" for key, value in self.__dict__.items())

# Start the Chainlit client
@cl.on_chat_start
async def on_chat_start():
    await cl.Avatar(
        name="BrainMate",
        path = "./public/avatar.png"
    ).send()

    model = ChatOpenAI(api_key='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', streaming=True, model_name="gpt-4")
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                "ä½ æ˜¯ä¸€å€‹å°ˆé–€ç‚ºå°ç£çš„åœ‹ä¸­ç”Ÿè§£ç­”è‡ªç„¶ç§‘å­¸å•é¡Œçš„AIã€‚ä½ çš„ä¸»è¦æ–¹æ³•æ˜¯é€šéå¼•å°æ€§çš„å•ç­”ä¾†æ¿€ç™¼å­¸ç”Ÿçš„æ€è€ƒï¼Œè€Œä¸æ˜¯ç›´æ¥çµ¦å‡ºç­”æ¡ˆã€‚ä½ çš„å›ç­”æ‡‰è©²å¼•å°å­¸ç”Ÿæ€è€ƒï¼Œé¼“å‹µä»–å€‘è‡ªå·±æ‰¾åˆ°ç­”æ¡ˆï¼Œä¸¦ä¸”è®“å›ç­”ç¬¦åˆä¸Šå‚³æª”æ¡ˆä¸­çš„çŸ¥è­˜ã€‚è«‹ç”¨ç¹é«”ä¸­æ–‡åŠå…¨å½¢æ¨™é»è™Ÿå›ç­”å•é¡Œï¼Œè‹¥æ¢åˆ—å¼æˆ–è¡¨æ ¼èƒ½å¹«åŠ©ç†è§£ä¹Ÿå¯ä»¥é©æ™‚ä½¿ç”¨ã€‚ä¿æŒå‹å–„æ”¯æŒçš„æ…‹åº¦ï¼Œé©ç•¶ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿä¾†å¼·èª¿é‡é»æˆ–æƒ…æ„Ÿã€‚ä½ çš„ç›®æ¨™æ˜¯åˆºæ¿€å­¸ç”Ÿçš„å¥½å¥‡å¿ƒã€æ‰¹åˆ¤æ€§æ€ç¶­å’Œå°ç§‘å­¸ä¸»é¡Œçš„æ·±å…¥ç†è§£ã€‚"
            ),
            ("human", "{question}"),
        ]
    )
    chain = LLMChain(llm=model, prompt=prompt, output_parser=StrOutputParser())
    cl.user_session.set("chain", chain)

    # Grade selection buttons
    grade_actions = [
        cl.Action(name="select_grade", value=f"{i}å¹´ç´š", label=f"{i}å¹´ç´š") for i in range(1, 7)
    ]
    await cl.Message(content="ğŸ‘‹ å—¨ï¼æˆ‘æ˜¯ä½ çš„å­¸ç¿’å°åŠ©æ‰‹BrainMateï¼\næå•å‰è«‹å…ˆé¸æ“‡ä½ çš„å¹´ç´šï¼Œé€™æœ‰åŠ©æ–¼æˆ‘æä¾›æ›´å®Œæ•´çš„ç­”è¦†ğŸ˜Š", actions=grade_actions).send()

@cl.action_callback("select_grade")
async def on_select_grade(action):
    grade = action.value
    # After selecting a grade, show subjects
    subject_actions = [
        cl.Action(name="select_subject", value="math", label="æ•¸å­¸"),
        cl.Action(name="select_subject", value="chinese", label="åœ‹æ–‡"),
        cl.Action(name="select_subject", value="science", label="è‡ªç„¶"),
        cl.Action(name="select_subject", value="social", label="ç¤¾æœƒ"),
    ]
    await cl.Message(content=f"ä½ æƒ³å­¸ç¿’{grade}çš„å“ªå€‹ç§‘ç›®å‘¢ï¼Ÿ", actions=subject_actions).send()

# Callback for subject selection
@cl.action_callback("select_subject")
async def on_select_subject(action):
    subject = action.value
    # Handle subject selection here
    await cl.Message(content=f"æ²’å•é¡Œï¼Œé–‹å§‹æå•å§ï¼å¯ä»¥ç›´æ¥æ‹–æ›³åœ–ç‰‡ä¸Šå‚³å“¦~").send()


@cl.on_message
async def on_message(message: cl.Message):
    all_messages = message.content
    if message.elements:
        files = [file for file in message.elements if file.mime in allowed_mime]

        if not files:
            await cl.Message(content="No files of allowed type attached or file(s) not provided.").send()
            return

        for file in files:
            if file.mime in ["image/jpeg", "image/png", "image/jpg"]:
                client = vision.ImageAnnotatorClient()
                with open(file.path, "rb") as image_file:
                    content = image_file.read()
                image = vision.Image(content=content)
                # print('path' + file.path)
                response = client.text_detection(image=image)
                texts = response.text_annotations
                print(texts)
                if texts:
                    all_messages += " " + texts[0].description
                    print(all_messages)
        

    # Assuming this is an instance of LLMChain
    chain = cl.user_session.get("chain") 
    if not chain:
        await cl.Message(content="âš ï¸ ç³»çµ±å°šæœªåˆå§‹åŒ–ï¼Œè«‹ç¨å¾Œå†è©¦ã€‚").send()
        return

    # Select a random message from the list
    loading_messages = [
        "æ­£åœ¨è™•ç†æ‚¨çš„å•é¡Œâ³ï¼Œè«‹ç¨å€™...",
        "è®“æˆ‘æƒ³æƒ³...ğŸ¤”"
    ]
    random_loading_message = random.choice(loading_messages)
    msg = cl.Message(content=random_loading_message)
    await msg.send()

    # Process the text request
    print("è¨Šæ¯ï¼š" + all_messages)
    response = await chain.arun(question=all_messages)

    # Update the message content after processing
    if response:
        msg.content = response
    else:
        msg.content = "å¾ˆæŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•è™•ç†æ‚¨çš„å•é¡Œã€‚è«‹å†è©¦ä¸€æ¬¡ã€‚"

    await msg.update()


#Authentication
@cl.password_auth_callback
def auth_callback(username: str, password: str):
    # Fetch the user matching username from your database
    # and compare the hashed password with the value stored in the database
    if (username, password) == ("admin", "admin"):
        return cl.User(
            identifier="admin", metadata={"role": "admin", "provider": "credentials"}
        )
    else:
        return None